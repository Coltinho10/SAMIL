{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Coltinho10/SAMIL/blob/main/notebook/DraftDoc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e47a1ab-850f-41cb-bfa1-f540c596ac34",
      "metadata": {
        "id": "5e47a1ab-850f-41cb-bfa1-f540c596ac34"
      },
      "source": [
        "# CS 598 Project Draft\n",
        "\n",
        "### Saturday, April 6 2024\n",
        "\n",
        "### Colton Bailey, Caleb Thomas, Indranil Ghosh\n",
        "<hr>\n",
        "\n",
        "https://github.com/Coltinho10/SAMIL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60cc1ec1-3329-4537-868d-9ad9f4f93add",
      "metadata": {
        "id": "60cc1ec1-3329-4537-868d-9ad9f4f93add"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4368089f-8299-46ee-aaa6-4e6149a9e04e",
      "metadata": {
        "id": "4368089f-8299-46ee-aaa6-4e6149a9e04e"
      },
      "source": [
        "The original paper titled \"Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning\" by Huang et al. introduces an innovative approach to detecting heart disease using ultrasound images.\n",
        "\n",
        "The authors addressed the difficulties of detecting heart disease though ultrasound images, why heart disease is a widespread issue throughout the world, and how early detection is necessary for more effective treatment and prognoses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ab07069-c9b7-49d5-ad19-46ec3fb6cbe0",
      "metadata": {
        "id": "8ab07069-c9b7-49d5-ad19-46ec3fb6cbe0"
      },
      "source": [
        "## Scope of Reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07b23717-e741-4a4d-9790-2664d3cc58bd",
      "metadata": {
        "id": "07b23717-e741-4a4d-9790-2664d3cc58bd"
      },
      "source": [
        "The scope of reproducibility for this [study](https://arxiv.org/abs/2306.00003) requires multiple steps, including:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e86d17ae-9e73-4bbc-b400-6de485bf820d",
      "metadata": {
        "id": "e86d17ae-9e73-4bbc-b400-6de485bf820d"
      },
      "source": [
        "### Data access\n",
        "\n",
        "Accessing the [TMED2 dataset](https://tmed.cs.tufts.edu/tmed_v2.html) from the original authors is the foremost important step in an attempt to reproduce the work previously done. Having access to the same dataset ensures consistency in the model's performance while training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc6d0099-5a1e-4b48-bea8-2d546a789eba",
      "metadata": {
        "id": "bc6d0099-5a1e-4b48-bea8-2d546a789eba"
      },
      "source": [
        "### Python virtual environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c94ac35-6ec4-4f7e-b35d-c6c732f97c17",
      "metadata": {
        "id": "8c94ac35-6ec4-4f7e-b35d-c6c732f97c17"
      },
      "source": [
        "Establishing the correct Python virtual interpreter and environment were also necessary to ensure all the required packages were installed and present.\n",
        "\n",
        "For this project, the Python `3.11.5` interpreter was chosen and found the most success out of the box.\n",
        "\n",
        "Missing dependencies can be installed with `pip` or `conda`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WuTdKCIE8E49",
      "metadata": {
        "id": "WuTdKCIE8E49"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5JN4Y5fs7GoQ",
      "metadata": {
        "id": "5JN4Y5fs7GoQ"
      },
      "outputs": [],
      "source": [
        "!python -V\n",
        "!pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 --index https://download.pytorch.org/whl/ --upgrade-strategy only-if-needed\n",
        "#!pip install pandas==2.2.1 pillow==10.2.0 scikit-learn==1.4.1.post1 scipy==1.13.0 tqdm==4.66.2 --upgrade-strategy only-if-needed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1085086d-4363-484f-8fb7-fb8f9f9f2fde",
      "metadata": {
        "id": "1085086d-4363-484f-8fb7-fb8f9f9f2fde"
      },
      "source": [
        "### Addressing platform compatibility"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60dced15-5b50-4f72-8e3d-26ac361b513c",
      "metadata": {
        "id": "60dced15-5b50-4f72-8e3d-26ac361b513c"
      },
      "source": [
        "For users on Apple's M1/M2 chipsets, Apple Silicon, additional steps are needed to train the model locally due to the ARM instruction set. This involves writing additional code (see below) to target the onboard GPUs that is not CUDA type. Fortunately, Apple and PyTorch provide a new package named Metal to [accelerate PyTorch training on Mac](https://developer.apple.com/metal/pytorch/).\n",
        "\n",
        "Due to this some additional requirements are needed to be met:\n",
        "- Mac computers with Apple silicon or AMD GPUs\n",
        "- macOS 12.3 or later\n",
        "- Python 3.7 or later"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example code from src/SAMIL/main.py\n",
        "\"\"\"\n",
        "from torch.cuda import is_available\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    cuda = None\n",
        "    if torch.cuda.is_available():\n",
        "        cuda = torch.cuda.is_available()\n",
        "    elif torch.backends.mps.is_available():\n",
        "        mps_device = torch.backends.mps.is_available()\n",
        "    if cuda:\n",
        "        print('cuda available')\n",
        "        device = torch.device('cuda')\n",
        "        args.device = device\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "    elif mps_device:\n",
        "        print('Metal GPU available')\n",
        "        device = torch.device('mps')\n",
        "        args.device = device\n",
        "    else:\n",
        "        raise ValueError('Not Using GPU?')\n",
        "\"\"\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "6JTx7C-5Gv2p"
      },
      "id": "6JTx7C-5Gv2p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "572761bc-0a96-4f50-8e8f-5986107ff814",
      "metadata": {
        "id": "572761bc-0a96-4f50-8e8f-5986107ff814"
      },
      "source": [
        "### Adapting code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "393cb680-0ba2-451d-8b33-224be0c3f8fe",
      "metadata": {
        "id": "393cb680-0ba2-451d-8b33-224be0c3f8fe"
      },
      "source": [
        "Additional debugging may be needed, primarily due to differences in package versions, as well as the dataset changing format from `.npy` to `.png`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07496b97-ee34-4bd0-bda3-7cb55a71673c",
      "metadata": {
        "id": "07496b97-ee34-4bd0-bda3-7cb55a71673c"
      },
      "source": [
        "### Initiating training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c5de0bb-87e2-4349-a74d-5bd745eb84d6",
      "metadata": {
        "id": "7c5de0bb-87e2-4349-a74d-5bd745eb84d6"
      },
      "source": [
        "Once completed with the previous steps (if applicable), initiating the training process using the [codebase](https://github.com/tufts-ml/SAMIL) and TMED2 dataset is the next step. This involves running the training pipeline to train the model on the dataset of labeled and unlabeled ultrasound images."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ppkNT9Oi4ALe",
      "metadata": {
        "id": "ppkNT9Oi4ALe"
      },
      "source": [
        "## Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "025892f2",
      "metadata": {
        "id": "025892f2"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f766439d",
      "metadata": {
        "id": "f766439d"
      },
      "source": [
        "#### Data descriptions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "576abb8b",
      "metadata": {
        "id": "576abb8b"
      },
      "source": [
        "**Source of the data**: The Tufts Medical Echocardiogram Dataset TMED-2 (Huang et al., 2022), is a collection of 2D echocardiogram images gathered during routine care at Tufts Medical Center in Boston, MA, USA from 2016-2021.\n",
        "\n",
        "**Statistics**:\n",
        "view_and_diagnosis_labeled_set : 599 studies from 577 unique patients (some patients have multiple studies on distinct days).\n",
        "The patients are partitioned into \"splits\" of 360 training, 119 validation, and 120 test studies.\n",
        "\n",
        "**view_labeled_set** : 705 studies from 703 unique patients\n",
        "\n",
        "**unlabeled_set** : 5486 studies from 5287 patients\n",
        "\n",
        "The data is stored in two top-level directory:\n",
        "\n",
        "1) labels : stored in comma-separated-value (CSV) plain-text files\n",
        "\n",
        "2) images : stored within folders as 112x112 pixel grayscale PNG files\n",
        "\n",
        "Labels and assignments of the labeled set to different train/validation/test splits are stored in the following CSV files in the top-level directory. Each CSV file has a row for each image file in the dataset, providing the relevant labels. The CSV file specs are as followed -\n",
        "\n",
        "1) labels_per_image.csv : CSV file with one row per image.\n",
        "\n",
        "- query_key\n",
        "Filename of specific image, as a string. Example: \"2977s1_0.png\".\n",
        "\n",
        "- view_label\n",
        "View label, as a string. Options: {\"PLAX\", \"PSAX\", \"A4C\", \"A2C\", \"A4CorA2CorOther\"}\n",
        "\n",
        "- diagnosis_label\n",
        "Diagnostic severity label, as a string. Options: {\"no_as\", \"mild_as\", \"mildtomoderate_AS\", \"moderate_AS\", \"severe_AS\", \"Not_Provided\"}\n",
        "\n",
        "2) TMED2_foldX_labeledpart.csv : CSV file with one row per image. Integer X denotes the specific train/valid/test split, and could take values in {0, 1, 2}\n",
        "\n",
        "- query_key\n",
        "Filename of specific image, as a string. Example: \"2977s1_0.png\".\n",
        "\n",
        "- view_classifier_split\n",
        "String that indicates which standard data split for image-level view classifier this image belongs to within fold X. Options: {\"train\", \"val\", \"test\"}.\n",
        "\n",
        "- diagnosis_classifier_split\n",
        "String that indicates which standard data split for image-level diagnosis classifier this image belongs to within fold X. Options: {\"train\", \"val\", \"test\", \"not_used\"}.\n",
        "\n",
        "- view_label\n",
        "View label, as a string. Options: {\"PLAX\", \"PSAX\", \"A4C\", \"A2C\", \"A4CorA2CorOther\"}\n",
        "\n",
        "- diagnosis_label\n",
        "Diagnostic severity label, as a string. Options: {\"no_as\", \"mild_as\", \"mildtomoderate_AS\", \"moderate_AS\", \"severe_AS\", \"Not_Provided\"}\n",
        "\n",
        "- SourceFolder\n",
        "Diagnostic severity label, as a string. Options: {\"no_as\", \"mild_as\", \"mildtomoderate_AS\", \"moderate_AS\", \"severe_AS\", \"Not_Provided\"}\n",
        "\n",
        "3) TMED2_foldX_unlabeledpart.csv : CSV file with one row per image. Integer X denotes the specific train/valid/test split, and could take values in {0, 1, 2} (these splits correspond exactly to the 3 splits used in our paper's experiments).\n",
        "\n",
        "- query_key\n",
        "Filename of specific image, as a string. Example: \"2977s1_0.png\".\n",
        "\n",
        "- SourceFolder\n",
        "Folder that the image is located\n",
        "\n",
        "\n",
        "The Images are stored within a hierarchy of folders such as below:\n",
        "\n",
        "view_and_diagnosis_labeled_set/labeled/\n",
        "view_and_diagnosis_labeled_set/unlabeled/\n",
        "view_labeled_set/labeled/\n",
        "view_labeled_set/unlabeled/\n",
        "unlabeled_set/unlabeled/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WSdgEa8H6dlr",
      "metadata": {
        "id": "WSdgEa8H6dlr"
      },
      "source": [
        "#### Implementation Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "VUM45vKQ5_yt",
      "metadata": {
        "id": "VUM45vKQ5_yt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing the Dataset\n",
        "1. Open the Google Drive link: https://drive.google.com/drive/folders/1n31WeVVK7qU7k1pF1WIDRxT-bMJjGubK?usp=drive_link with a @illinois.edu account.\n",
        "\n",
        "2. Right click the **SAMIL_colab_data_1** folder, in the options select Organize/Add Shortcut, and then add the shortcut to **MyDrive**."
      ],
      "metadata": {
        "id": "-CUEVLa4GeOQ"
      },
      "id": "-CUEVLa4GeOQ"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "pgnCAd3RjQRQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pgnCAd3RjQRQ",
        "outputId": "e51e840a-4fff-4764-80a7-f518f2f6a63d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/.shortcut-targets-by-id/1n31WeVVK7qU7k1pF1WIDRxT-bMJjGubK/SAMIL_colab_data_1\n",
            "checkpoint  data  info\tresults  src\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/.shortcut-targets-by-id/1n31WeVVK7qU7k1pF1WIDRxT-bMJjGubK/SAMIL_colab_data_1\"\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c26d68b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "c26d68b2",
        "outputId": "adce3934-fa7f-4876-ec3f-cd2201af0fb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing [4] studies\n",
            "Processing [4] studies\n",
            "Processing [4] studies\n"
          ]
        }
      ],
      "source": [
        "from src.dataset import EchoDataset\n",
        "import src.models as models\n",
        "import src.utils as utils\n",
        "\n",
        "@dataclass()\n",
        "class TrainingArgs():\n",
        "    patience=200\n",
        "    batch_size=1\n",
        "    num_workers=2\n",
        "    dataset_name='echo'\n",
        "    training_seed=0\n",
        "    development_size='DEV479'\n",
        "    train_epoch=10\n",
        "    start_epoch=0\n",
        "    eval_every_Xepoch=1\n",
        "    script=\"src.SAMIL.main\"\n",
        "    use_data_normalization=False\n",
        "    augmentation='RandAug'\n",
        "    sampling_strategy='first_frame'\n",
        "    use_class_weights=True\n",
        "    class_weights = [0.25, 0.25, 0.25]\n",
        "    Pretrained='Whole'\n",
        "    ViewRegularization_warmup_schedule_type='Linear'\n",
        "    ViewRegularization_warmup_pos = 0.4\n",
        "    optimizer_type='SGD'\n",
        "    nesterov = \"store_true\"\n",
        "    lr_schedule_type='CosineLR'\n",
        "    lr_warmup_epochs = 0\n",
        "    lr_cycle_epochs=5\n",
        "\n",
        "    ema_decay = 0.999\n",
        "\n",
        "    checkpoint_dir = \"checkpoint\"\n",
        "    data_info_dir = \"info\"\n",
        "    data_dir = \"data\"\n",
        "\n",
        "    assert torch.cuda.is_available(), \"Need GPU\"\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    data_seed = 0\n",
        "    train_dir = f\"training\"\n",
        "    train_PatientStudy_list_path = f\"{data_info_dir}/train_studies.csv\"\n",
        "    val_PatientStudy_list_path = f\"{data_info_dir}/val_studies.csv\"\n",
        "    test_PatientStudy_list_path = f\"{data_info_dir}/test_studies.csv\"\n",
        "\n",
        "    resume='last_checkpoint.pth.tar'\n",
        "    resume_checkpoint_fullpath = f\"{checkpoint_dir}/{resume}\"\n",
        "\n",
        "    lr=0.0005\n",
        "    wd=0.0001\n",
        "    lambda_ViewRegularization=20\n",
        "    T=0.05\n",
        "\n",
        "args = TrainingArgs()\n",
        "utils.set_seed(args.training_seed)\n",
        "\n",
        "transform_eval = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "transform_labeledtrain = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(\n",
        "        size=112, padding=int(112*0.125), padding_mode='reflect'\n",
        "        ),\n",
        "    utils.RandAugmentMC(n=2, m=10),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_PatientStudy_list = pd.read_csv(args.train_PatientStudy_list_path)\n",
        "train_PatientStudy_list = train_PatientStudy_list['study'].values\n",
        "val_PatientStudy_list = pd.read_csv(args.val_PatientStudy_list_path)\n",
        "val_PatientStudy_list = val_PatientStudy_list['study'].values\n",
        "test_PatientStudy_list = pd.read_csv(args.test_PatientStudy_list_path)\n",
        "test_PatientStudy_list = test_PatientStudy_list['study'].values\n",
        "\n",
        "TMED2SummaryTable = pd.read_csv(os.path.join(args.data_info_dir, 'TMED2SummaryTable.csv'))\n",
        "\n",
        "train_dataset = EchoDataset(\n",
        "    train_PatientStudy_list,\n",
        "    TMED2SummaryTable,\n",
        "    args.data_dir,\n",
        "    sampling_strategy=args.sampling_strategy,\n",
        "    training_seed=args.training_seed,\n",
        "    transform_fn=transform_labeledtrain\n",
        "    )\n",
        "val_dataset = EchoDataset(\n",
        "    val_PatientStudy_list,\n",
        "    TMED2SummaryTable,\n",
        "    args.data_dir,\n",
        "    sampling_strategy='first_frame',\n",
        "    training_seed=args.training_seed,\n",
        "    transform_fn=transform_eval\n",
        "    )\n",
        "test_dataset = EchoDataset(\n",
        "    test_PatientStudy_list,\n",
        "    TMED2SummaryTable,\n",
        "    args.data_dir,\n",
        "    sampling_strategy='first_frame',\n",
        "    training_seed=args.training_seed,\n",
        "    transform_fn=transform_eval\n",
        "    )\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers\n",
        "    )\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers\n",
        "    )\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AXLvSig9MqPr",
      "metadata": {
        "id": "AXLvSig9MqPr"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C9GVA7PG6kXl",
      "metadata": {
        "id": "C9GVA7PG6kXl"
      },
      "source": [
        "#### Model descriptions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "axFDvyk687AD",
      "metadata": {
        "id": "axFDvyk687AD"
      },
      "source": [
        "The authors have used Supervised Attention Multiple Instance Learning (SAMIL) model. This framework integrates multiple views of ultrasound images to detect heart disease. In Multiple Instance Learning (MIL) framework, instead of labeling individual images, only a set of (bags) of instances are labeled. Here, a set or a bag represents a whole ultrasound image, and the model learns to make predictions based on the presence or absence of heart disease in each image.\n",
        "\n",
        "From a set of images of unknown and diverse views, a feature extractor processes each image individually into an embedding vector. On these, two attention modules are applied - 1)one supervised by a view classifier, and 2) one without. These modules produce attention weights for each instance. The final study representation averages the image embeddings by combining the two attentions.\n",
        "\n",
        "Through the SAMIL framework, the model learns to distinguish between positive instances with heart disease and negative instances without heart disease."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xod7rMUF6mHZ",
      "metadata": {
        "id": "xod7rMUF6mHZ"
      },
      "source": [
        "#### Implementation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "hxRLQCzEMrfj",
      "metadata": {
        "id": "hxRLQCzEMrfj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4625f0e6-7b05-4e3f-872a-174d320ab0f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sending to: cuda\n",
            "self.param_keys: ['feature_extractor_part1.0.weight', 'feature_extractor_part1.0.bias', 'feature_extractor_part1.3.weight', 'feature_extractor_part1.3.bias', 'feature_extractor_part1.6.weight', 'feature_extractor_part1.6.bias', 'feature_extractor_part1.9.weight', 'feature_extractor_part1.9.bias', 'feature_extractor_part2.0.weight', 'feature_extractor_part2.0.bias', 'feature_extractor_part3.0.weight', 'feature_extractor_part3.0.bias', 'feature_extractor_part3.2.weight', 'feature_extractor_part3.2.bias', 'attention_V.0.weight', 'attention_V.0.bias', 'attention_V.2.weight', 'attention_V.2.bias', 'attention_U.0.weight', 'attention_U.0.bias', 'attention_U.2.weight', 'attention_U.2.bias', 'classifier.0.weight', 'classifier.0.bias']\n",
            "self.buffer_keys: []\n"
          ]
        }
      ],
      "source": [
        "class SAMIL(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(SAMIL, self).__init__()\n",
        "        self.L = 500\n",
        "        self.B = 250\n",
        "        self.D = 128\n",
        "        self.K = 1\n",
        "        self.num_classes = num_classes\n",
        "        self.feature_extractor_part1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 20, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(20, 50, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "             #hz added\n",
        "            nn.Conv2d(50, 100, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.Conv2d(100, 200, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "        )\n",
        "        self.feature_extractor_part2 = nn.Sequential(\n",
        "            nn.Linear(200 * 4 * 4, self.L),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.feature_extractor_part3 = nn.Sequential(\n",
        "\n",
        "            nn.Linear(self.L, self.B),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.B, self.L),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.attention_V = nn.Sequential(\n",
        "            nn.Linear(self.L, self.D),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.D, self.K)\n",
        "        )\n",
        "        self.attention_U = nn.Sequential(\n",
        "            nn.Linear(self.L, self.D),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.D, self.K)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.L*self.K, self.num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.squeeze(0)\n",
        "        H = self.feature_extractor_part1(x)\n",
        "        H = H.view(-1, 200 * 4 * 4)\n",
        "        H = self.feature_extractor_part2(H)  # NxL\n",
        "        A_V = self.attention_V(H)  # NxK\n",
        "        A_V = torch.transpose(A_V, 1, 0)  # KxN\n",
        "        A_V = F.softmax(A_V, dim=1)  # softmax over N\n",
        "        H = self.feature_extractor_part3(H)\n",
        "        A_U = self.attention_U(H)  # NxK\n",
        "        A_U = torch.transpose(A_U, 1, 0)\n",
        "        A_U = F.softmax(A_U, dim=1)\n",
        "        A = torch.exp(torch.log(A_V) + torch.log(A_U))\n",
        "        A = A/torch.sum(A)\n",
        "        M = torch.mm(A, H)\n",
        "        out = self.classifier(M)\n",
        "        return out, A_V\n",
        "\n",
        "\n",
        "weights = args.class_weights\n",
        "weights = torch.Tensor(weights)\n",
        "weights = weights.to(args.device)\n",
        "\n",
        "print(\"Sending to:\", args.device)\n",
        "\n",
        "view_model = models.create_view_model(args)\n",
        "view_model.to(args.device)\n",
        "\n",
        "model = SAMIL()\n",
        "model.to(args.device)\n",
        "\n",
        "no_decay = ['bias', 'bn']\n",
        "grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(\n",
        "        nd in n for nd in no_decay)], 'weight_decay': args.wd},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(\n",
        "        nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    grouped_parameters, lr=args.lr, momentum=0.9, nesterov=args.nesterov\n",
        "    )\n",
        "scheduler = utils.get_cosine_schedule_with_warmup(\n",
        "    optimizer, args.lr_warmup_epochs, args.lr_cycle_epochs\n",
        "    )\n",
        "ema_model = models.ModelEMA(args, model, args.ema_decay)\n",
        "args.start_epoch = 0\n",
        "\n",
        "assert os.path.isfile(args.resume_checkpoint_fullpath)\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    checkpoint = torch.load(\n",
        "        args.resume_checkpoint_fullpath, map_location=torch.device('cpu')\n",
        "        )\n",
        "else:\n",
        "    checkpoint = torch.load(args.resume_checkpoint_fullpath)\n",
        "\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "ema_model.ema.load_state_dict(checkpoint['ema_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "scheduler.load_state_dict(checkpoint['scheduler'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FFJ1zniyODKq",
      "metadata": {
        "id": "FFJ1zniyODKq"
      },
      "source": [
        "### Training\n",
        "\n",
        "Our best model was trained for 594 epochs, and used the following during training:\n",
        "1. Stochastic Gradient Descent was used for the optimizer.\n",
        "2. The loss is a combination of minimizing the Cross-Entropy loss between the observed diagnosis labels and the predicted probabilities for a bag of training images.The second part of the loss is minimizing the KL-divergence between the\n",
        "relevance scores (given by the view classifier) and the attention weights. The final piece of the loss equation is the hyperparameter, Lambda View Regularization. It determines the weight that is given to the KL-divergence term.\n",
        "3. At the end of each epoch the model is evaluated using a validation set. This validation set serves two purposes. The first is that it allows Early Stopping to be used. If the validation balance accuracy hasn't peaked in a set number of epochs the model's training is stopped. Secondly, it allows for checkpoints of the model to be saved when a new peak validation balance accuracy is reached."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RxlneTdu60Ug",
      "metadata": {
        "id": "RxlneTdu60Ug"
      },
      "source": [
        "#### Computational requirements\n",
        "\n",
        "The total training time to train the model for 594 epochs was ~14 hours. So a hardware configuration similar to the following setup is recommended:\n",
        "\n",
        "1. CPU: 13th Gen Intel(R) Core(TM) i7-13700K, 16 cores, 5400 MHz base clock speed, offering substantial parallel processing capabilities suitable for deep learning tasks.\n",
        "2. RAM: 32 GB of RAM, providing sufficient memory for loading large batches of data during training and reducing data transfer overhead.\n",
        "3. GPU: NVIDIA RTX 4070 Super Ti with 16 GB of VRAM, offering powerful computational capabilities for accelerating deep learning computations, particularly matrix multiplications and convolutions commonly found in CNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NM_vPdbJ611x",
      "metadata": {
        "id": "NM_vPdbJ611x"
      },
      "source": [
        "#### Implementation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vxYecG4UOF4V",
      "metadata": {
        "id": "vxYecG4UOF4V"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(\n",
        "        args, weights, train_loader, model, ema_model, view_model, optimizer, scheduler, epoch\n",
        "        ):\n",
        "    model.train()\n",
        "    TotalLoss_this_epoch = []\n",
        "    LabeledCELoss_this_epoch = []\n",
        "    ViewRegularizationLoss_this_epoch = []\n",
        "    scaled_ViewRegularizationLoss_this_epoch = []\n",
        "    train_iter = iter(train_loader)\n",
        "    print(\"\\nNumber of Train Studies: \", len(train_loader))\n",
        "    n_steps_per_epoch = len(train_loader) #360 train studies, batch size 1\n",
        "    p_bar = tqdm(range(n_steps_per_epoch), disable=False)\n",
        "    for batch_idx in range(n_steps_per_epoch):\n",
        "        try:\n",
        "            data, bag_label = train_iter.next()\n",
        "        except:\n",
        "            train_iter = iter(train_loader)\n",
        "            data, bag_label = train_iter.next()\n",
        "        data, bag_label = data.to(args.device), bag_label.to(args.device)\n",
        "        outputs, attentions = model(data)\n",
        "        log_attentions = torch.log(attentions)\n",
        "        with torch.no_grad():\n",
        "            view_predictions = view_model(data.squeeze(0))\n",
        "            softmax_view_predictions = F.softmax(view_predictions, dim=1)\n",
        "            predicted_relevance = softmax_view_predictions[:, :2]\n",
        "            predicted_relevance = torch.sum(predicted_relevance, dim=1)\n",
        "            predicted_relative_relevance = F.softmax(predicted_relevance/args.T)\n",
        "            predicted_relative_relevance = predicted_relative_relevance.unsqueeze(0)\n",
        "        if args.use_class_weights:\n",
        "            LabeledCELoss = F.cross_entropy(outputs, bag_label, weights, reduction='mean')\n",
        "        else:\n",
        "            LabeledCELoss = F.cross_entropy(outputs, bag_label, reduction='mean')\n",
        "        assert args.ViewRegularization_warmup_schedule_type == 'Linear'\n",
        "        current_warmup = np.clip(\n",
        "            epoch/(float(args.ViewRegularization_warmup_pos) * args.train_epoch), 0, 1\n",
        "            )\n",
        "        ViewRegularizationLoss = F.kl_div(\n",
        "            input=log_attentions, target=predicted_relative_relevance, log_target=False, reduction='batchmean'\n",
        "            )\n",
        "        # backward pass\n",
        "        total_loss = LabeledCELoss + args.lambda_ViewRegularization * ViewRegularizationLoss * current_warmup\n",
        "        total_loss.backward()\n",
        "\n",
        "        TotalLoss_this_epoch.append(total_loss.item())\n",
        "        LabeledCELoss_this_epoch.append(LabeledCELoss.item())\n",
        "        ViewRegularizationLoss_this_epoch.append(ViewRegularizationLoss.item())\n",
        "        scaled_ViewRegularizationLoss_this_epoch.append(\n",
        "            args.lambda_ViewRegularization * ViewRegularizationLoss.item() * current_warmup\n",
        "            )\n",
        "        optimizer.step()\n",
        "        ema_model.update(model)\n",
        "        model.zero_grad()\n",
        "    scheduler.step()\n",
        "    return (\n",
        "        TotalLoss_this_epoch,\n",
        "        LabeledCELoss_this_epoch,\n",
        "        ViewRegularizationLoss_this_epoch,\n",
        "        scaled_ViewRegularizationLoss_this_epoch\n",
        "        )\n",
        "epochs, should_train = 2, False\n",
        "if should_train:\n",
        "    args.start_epoch, args.train_epoch = 0, epochs\n",
        "    train_loss_dict = dict()\n",
        "    train_loss_dict['Totalloss'] = []\n",
        "    train_loss_dict['LabeledCEloss'] = []\n",
        "    train_loss_dict['ViewRegularizationLoss'] = []\n",
        "    for epoch in tqdm(range(args.start_epoch, args.train_epoch)):\n",
        "        TotalLoss_list,\\\n",
        "        LabeledCEloss_list,\\\n",
        "        ViewRegularizationLoss_list,\\\n",
        "        scaled_ViewRegularizationLoss_list\\\n",
        "        = train_one_epoch(\n",
        "            args, weights, train_loader, model, ema_model, view_model, optimizer, scheduler, epoch\n",
        "            )\n",
        "        print(f\"Trained Epoch: {epoch}\")\n",
        "        train_loss_dict['Totalloss'].extend(TotalLoss_list)\n",
        "        train_loss_dict['LabeledCEloss'].extend(LabeledCEloss_list)\n",
        "        train_loss_dict['ViewRegularizationLoss'].extend(ViewRegularizationLoss_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RnNHbwbDQLtq",
      "metadata": {
        "id": "RnNHbwbDQLtq"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gJiZcaVh6-5h",
      "metadata": {
        "id": "gJiZcaVh6-5h"
      },
      "source": [
        "#### Metrics descriptions\n",
        "\n",
        "The authors use balance accuracy as their primary performance metric, and we do the same. The reason for this is because of the class imbalance in the TMED-2 dataset. Balance accuracy involves taking the sum of the true positives for each class divided by the total number of examples with that class."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LoP7SLP37D2Q",
      "metadata": {
        "id": "LoP7SLP37D2Q"
      },
      "source": [
        "#### Implementation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scg9_EnyQNGw",
      "metadata": {
        "id": "scg9_EnyQNGw"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix as sklearn_cm\n",
        "\n",
        "def calculate_balanced_accuracy(prediction, true_target, return_type = 'only balanced_accuracy'):\n",
        "    soft = F.softmax(torch.Tensor(prediction), dim=1)\n",
        "    confusion_matrix = sklearn_cm(true_target, torch.argmax(soft, dim=1).numpy())\n",
        "    n_class = confusion_matrix.shape[0]\n",
        "    assert n_class==3\n",
        "    recalls = []\n",
        "    for i in range(n_class):\n",
        "        recall = confusion_matrix[i,i]/np.sum(confusion_matrix[i])\n",
        "        recalls.append(recall)\n",
        "        print('class{} recall: {}'.format(i, recall), flush=True)\n",
        "    balanced_accuracy = np.mean(np.array(recalls))\n",
        "    if return_type == 'all':\n",
        "        return balanced_accuracy * 100, recalls\n",
        "    return balanced_accuracy * 100\n",
        "\n",
        "def eval_model(args, data_loader, raw_model, ema_model):\n",
        "    raw_model.eval()\n",
        "    ema_model.eval()\n",
        "    data_loader = tqdm(data_loader, disable=False)\n",
        "    with torch.no_grad():\n",
        "        total_targets = []\n",
        "        total_raw_outputs = []\n",
        "        total_ema_outputs = []\n",
        "        for batch_idx, (data, bag_label) in enumerate(data_loader):\n",
        "            data, bag_label = data.to(args.device), bag_label.to(args.device)\n",
        "            raw_outputs, raw_attention_weights = raw_model(data)\n",
        "            ema_outputs, ema_attention_weights = ema_model(data)\n",
        "            total_targets.append(bag_label.detach().cpu())\n",
        "            total_raw_outputs.append(raw_outputs.detach().cpu())\n",
        "            total_ema_outputs.append(ema_outputs.detach().cpu())\n",
        "\n",
        "        total_targets = np.concatenate(total_targets, axis=0)\n",
        "        total_raw_outputs = np.concatenate(total_raw_outputs, axis=0)\n",
        "        total_ema_outputs = np.concatenate(total_ema_outputs, axis=0)\n",
        "        raw_Bacc = calculate_balanced_accuracy(total_raw_outputs, total_targets)\n",
        "        ema_Bacc = calculate_balanced_accuracy(total_ema_outputs, total_targets)\n",
        "\n",
        "    return raw_Bacc, ema_Bacc, total_targets, total_raw_outputs, total_ema_outputs\n",
        "\n",
        "val_score = eval_model(args, val_loader, model, ema_model.ema)\n",
        "test_score = eval_model(args, test_loader, model, ema_model.ema)\n",
        "print(f\"Validation Raw Balance Acc Score: \", val_score[0])\n",
        "print(f\"Test Raw Balance Acc Score: \", test_score[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1h69L761sWAK",
      "metadata": {
        "id": "1h69L761sWAK"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pyT3UOqV5AEF",
      "metadata": {
        "id": "pyT3UOqV5AEF"
      },
      "source": [
        "#### Results\n",
        "\n",
        "Our best SAMIL model trained for a total of 594 epochs with the following hyperparameters:\n",
        "1. Optimizer = SGD\n",
        "2. Learning Rate = 0.0005\n",
        "3. Weight Decay = 0.0001\n",
        "4. T = 0.1\n",
        "5. Lambda View Regularization = 20.0\n",
        "\n",
        "The model demonstrated promising performance, achieving a test balance accuracy of 70.5%. While this result is sufficient for the scope of this project, it falls slightly short of the 75.4% accuracy reported in the original paper.\n",
        "\n",
        "Analysis of the total loss over epochs reveals intriguing insights into the model's performance. Initially, from epoch 0 to around 300, there was a consistent decline in the total loss, indicating effective learning and convergence towards an optimal solution. However, beyond epoch 500, the total loss exhibited a trend of maintaining a higher threshold. This shift suggests potential overfitting of the dataset, where the model begins to memorize the training data rather than generalizing well to unseen examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gQ0PRJ975TEM",
      "metadata": {
        "id": "gQ0PRJ975TEM"
      },
      "source": [
        "#### Analyses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_raw_bacc_results = pd.read_csv(\"results/validation_bal_acc.csv\")\n",
        "test_raw_bacc_results = pd.read_csv(\"results/test_bal_acc.csv\")\n",
        "loss_results = pd.read_csv(\"results/loss.csv\")\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "ax = plt.subplot(1, 3, 1)\n",
        "ax.set_title(\"Validation Balance Accuracy\")\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"Raw Balance Accuracy\")\n",
        "plt.plot(val_raw_bacc_results[\"Step\"], val_raw_bacc_results[\"Value\"], linewidth=2)\n",
        "\n",
        "ax = plt.subplot(1, 3, 2)\n",
        "ax.set_title(\"Test Balance Accuracy\")\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"Raw Balance Accuracy\")\n",
        "plt.plot(test_raw_bacc_results[\"Step\"], test_raw_bacc_results[\"Value\"], linewidth=2)\n",
        "\n",
        "\n",
        "ax = plt.subplot(1, 3, 3)\n",
        "ax.set_title(\"Total Loss\")\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"Loss\")\n",
        "plt.plot(loss_results[\"Step\"], loss_results[\"Value\"], linewidth=2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "26dH1AD09wlj"
      },
      "id": "26dH1AD09wlj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "oYac83w05XW_",
      "metadata": {
        "id": "oYac83w05XW_"
      },
      "source": [
        "#### Plans"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plan to continue to tune the hyperparameters in an attempt to exceed the test balance accuracy acheived in the original paper.\n",
        "\n",
        "As previously mentioned in the results subsection, the total loss increases after ~epoch 500. Updating hyperparameters such as learning rate or dropout could potentially help mitigate this overfitting. We plan to do this through hyperparameter search using grid search to find the best set of hyperparameters.\n",
        "\n",
        "We also plan to introduce a more eager early stopping mechanism to prevent the model from overfitting the data. By monitoring the validation loss and stopping training when it increases over a period of time or by a certain amount, this will prevent from overfitting and lead to better performance on general datasets.\n",
        "\n",
        "Finally, data augmentation is something we have considered to increase the size of the dataset. Data augmentation can be a simple change to the images in the dataset that will greatly expand the size and diversity of the images, thus improving performance. We plan to augment the data by rotating and flipping the ultrasound images in the dataset."
      ],
      "metadata": {
        "id": "eFk7NSPUR6B1"
      },
      "id": "eFk7NSPUR6B1"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lVGXZKJgUhdW"
      },
      "id": "lVGXZKJgUhdW",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "025892f2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}